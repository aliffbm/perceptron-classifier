{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import training and test datasets {-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull in files and create dataframes\n",
    "train = np.loadtxt(\"pa3train.txt\")\n",
    "test = np.loadtxt(\"pa3test.txt\")\n",
    "\n",
    "\n",
    "\n",
    "# training data \n",
    "df_train = pd.DataFrame(train)\n",
    "df_train_2 = df_train[df_train.apply(lambda x: x[819] == 1 or x[819] == 2, axis=1)]\n",
    "#df_train_2 = df_train_2.sample(frac=1)\n",
    "# test data\n",
    "df_test = pd.DataFrame(test)\n",
    "df_test_2 = df_test[df_test.apply(lambda x: x[819] == 1 or x[819] == 2, axis=1)]\n",
    "#df_dict = pd.DataFrame(\"pa3dictionary.txt\")\n",
    "\n",
    "\n",
    "\n",
    "X_train = df_train.drop([819], axis=1)\n",
    "y_train = df_train[819]\n",
    "\n",
    "X_train_2 = df_train_2.drop([819], axis=1)\n",
    "y_train_2 = df_train_2[819]\n",
    "\n",
    "## mapping 1 -> 1 and 2 -> -1\n",
    "y_train_2_trans = y_train_2.apply(lambda x: 1 if x > 1 else -1)\n",
    "\n",
    "X_test = df_test.drop([819], axis=1)\n",
    "y_test = df_test[819]\n",
    "\n",
    "X_test_2 = df_test_2.drop([819], axis=1)\n",
    "y_test_2 = df_test_2[819]\n",
    "# mapping 1 -> 1 and 2 -> -1\n",
    "y_test_2_trans = y_test_2.apply(lambda x: 1 if x > 1 else -1)\n",
    "\n",
    "label = 819"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "377"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test_2_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Perceptron Class to train classifier {-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, rounds):\n",
    "        self.rounds = rounds\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.w = None\n",
    "        self.w_avg = None\n",
    "        self.array_of_w = []\n",
    "        self.predictions = []\n",
    "        self.voted_predictions = []\n",
    "        self.avg_predictions = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.w = pd.Series(np.arange(len(self.X.columns)))*0\n",
    "        m = 1\n",
    "        c = 1\n",
    "        for j in range(self.rounds):\n",
    "            for i in range(len(self.X)):\n",
    "                if self.y.iloc[i]*self.w.dot(self.X.iloc[i]) <= 0:\n",
    "                    self.w = self.w + self.y.iloc[i]*self.X.iloc[i]\n",
    "                    w = self.w\n",
    "                    m = m + 1 \n",
    "                    c = 1\n",
    "                    self.array_of_w.append([w, c])\n",
    "                else: \n",
    "                    c = c + 1\n",
    "                    index = len(self.array_of_w) - 1 # last position of last [w, c]\n",
    "                    self.array_of_w[index][1] = c\n",
    "    \n",
    "    def perceptron_get_sign(self, w, x):\n",
    "        return -1 if w.dot(x) <= 0 else 1\n",
    "    \n",
    "    def perceptron_get_sign_with_c(self, w, x, c):\n",
    "        return c*self.perceptron_get_sign(w,x)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        self.predictions = []\n",
    "        for i in range(len(X)):\n",
    "            x = X.iloc[i]\n",
    "            self.predictions.append(self.perceptron_get_sign(self.w, x))\n",
    "        return self.predictions\n",
    "    \n",
    "    def voted_predict(self, X):\n",
    "        self.voted_predictions = []\n",
    "        for i in range(len(X)):\n",
    "            x = X.iloc[i]\n",
    "            voted_sum = 0\n",
    "            for w_c in self.array_of_w:\n",
    "                w = w_c[0]\n",
    "                c = w_c[1]\n",
    "                voted_sum = voted_sum + self.perceptron_get_sign_with_c(w, x, c)\n",
    "\n",
    "            pred = -1 if voted_sum <= 0 else 1\n",
    "            self.voted_predictions.append(pred)\n",
    "\n",
    "        return self.voted_predictions\n",
    "    \n",
    "    def avg_predict(self, X):\n",
    "        self.avg_predictions = []\n",
    "        # get first w,c pair\n",
    "        w = self.array_of_w[0][0] # grab w\n",
    "        c = self.array_of_w[0][1] # grab c\n",
    "        w_avg = c*w # [0, ..., 0] + first c*w\n",
    "        for i in range(1, len(self.array_of_w)):\n",
    "                w = self.array_of_w[i][0]\n",
    "                c = self.array_of_w[i][1]\n",
    "                w_avg = w_avg + c*w\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            x = X.iloc[i]\n",
    "            sign = self.perceptron_get_sign(w_avg, x)\n",
    "            self.avg_predictions.append(sign)\n",
    "        \n",
    "        self.w_avg = w_avg\n",
    "\n",
    "        return self.avg_predictions\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getError(preds, y):\n",
    "    return 1 - ((preds == y).sum() / len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: {-}\n",
    "    What are the training errors and the test errors of perceptron, voted perceptron and averaged perceptron after two, three and four passes?\n",
    "### First, will create an instances of Perceptrons with 2, 3, and 4 rounds {-}\n",
    "### Then fit the training data to each one of these {-}\n",
    "##  The y labels have been mapped 1 -> -1 and 2 -> 1 {-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes one argument for the number of rounds \n",
    "\n",
    "# create for train set\n",
    "cls2 = Perceptron(2)\n",
    "cls3 = Perceptron(3)\n",
    "cls4 = Perceptron(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls2.fit(X_train_2, y_train_2_trans)\n",
    "cls3.fit(X_train_2, y_train_2_trans)\n",
    "cls4.fit(X_train_2, y_train_2_trans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get predictions for Perceptron and compute errors  {-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict for training data\n",
    "train_preds2 = cls2.predict(X_train_2)\n",
    "train_preds3 = cls3.predict(X_train_2)\n",
    "train_preds4 = cls4.predict(X_train_2)\n",
    "\n",
    "\n",
    "# predict for test data\n",
    "test_preds2 = cls2.predict(X_test_2)\n",
    "test_preds3 = cls3.predict(X_test_2)\n",
    "test_preds4 = cls4.predict(X_test_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors on Training data for Perceptron:\n",
      "Error for 2 rounds: 0.0357798165137615\n",
      "Error for 3 rounds: 0.01834862385321101\n",
      "Error for 4 rounds: 0.016513761467889854\n",
      "\n",
      "\n",
      "Errors on Test data for Perceptron:\n",
      "Error for 2 rounds: 0.06100795755968169\n",
      "Error for 3 rounds: 0.04509283819628651\n",
      "Error for 4 rounds: 0.04509283819628651\n"
     ]
    }
   ],
   "source": [
    "print(\"Errors on Training data for Perceptron:\\nError for 2 rounds: {}\\nError for 3 rounds: {}\\nError for 4 rounds: {}\"\n",
    "      .format(getError(train_preds2, y_train_2_trans), getError(train_preds3, y_train_2_trans), getError(train_preds4, y_train_2_trans)))\n",
    "\n",
    "print(\"\\n\\nErrors on Test data for Perceptron:\\nError for 2 rounds: {}\\nError for 3 rounds: {}\\nError for 4 rounds: {}\"\n",
    "      .format(getError(test_preds2, y_test_2_trans), getError(test_preds3, y_test_2_trans), getError(test_preds4, y_test_2_trans)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get predictions for voted perceptron and compute errors {-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict voted perceptron on training data\n",
    "train_voted_preds2 = cls2.voted_predict(X_train_2)\n",
    "train_voted_preds3 = cls3.voted_predict(X_train_2)\n",
    "train_voted_preds4 = cls4.voted_predict(X_train_2)\n",
    "\n",
    "\n",
    "\n",
    "# predict voted perceptron on test data\n",
    "test_voted_preds2 = cls2.voted_predict(X_test_2)\n",
    "test_voted_preds3 = cls3.voted_predict(X_test_2)\n",
    "test_voted_preds4 = cls4.voted_predict(X_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors on Training Data for Voted Perceptron:\n",
      "Error for 2 rounds: 0.038532110091743066\n",
      "Error for 3 rounds: 0.026605504587155937\n",
      "Error for 4 rounds: 0.022018348623853212\n",
      "\n",
      "\n",
      "Errors on Test Data for Voted Perceptron:\n",
      "Error for 2 rounds: 0.06100795755968169\n",
      "Error for 3 rounds: 0.04244031830238726\n",
      "Error for 4 rounds: 0.04509283819628651\n"
     ]
    }
   ],
   "source": [
    "print(\"Errors on Training Data for Voted Perceptron:\\nError for 2 rounds: {}\\nError for 3 rounds: {}\\nError for 4 rounds: {}\"\n",
    "      .format(getError(train_voted_preds2, y_train_2_trans), getError(train_voted_preds3, y_train_2_trans), getError(train_voted_preds4, y_train_2_trans)))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\\nErrors on Test Data for Voted Perceptron:\\nError for 2 rounds: {}\\nError for 3 rounds: {}\\nError for 4 rounds: {}\"\n",
    "      .format(getError(test_voted_preds2, y_test_2_trans), getError(test_voted_preds3, y_test_2_trans), getError(test_voted_preds4, y_test_2_trans)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get predictions for averaged perceptron and compute errors {-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_avg_preds2 = cls2.avg_predict(X_train_2)\n",
    "train_avg_preds3 = cls3.avg_predict(X_train_2)\n",
    "train_avg_preds4 = cls4.avg_predict(X_train_2)\n",
    "\n",
    "\n",
    "test_avg_preds2 = cls2.avg_predict(X_test_2)\n",
    "test_avg_preds3 = cls3.avg_predict(X_test_2)\n",
    "test_avg_preds4 = cls4.avg_predict(X_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors for Averaged Perceptron:\n",
      "Error for 2 rounds: 0.05137614678899083\n",
      "Error for 3 rounds: 0.034862385321100864\n",
      "Error for 4 rounds: 0.031192660550458662\n",
      "\n",
      "\n",
      "Errors for Averaged Perceptron:\n",
      "Error for 2 rounds: 0.08222811671087538\n",
      "Error for 3 rounds: 0.06100795755968169\n",
      "Error for 4 rounds: 0.050397877984084904\n"
     ]
    }
   ],
   "source": [
    "print(\"Errors for Averaged Perceptron:\\nError for 2 rounds: {}\\nError for 3 rounds: {}\\nError for 4 rounds: {}\"\n",
    "      .format(getError(train_avg_preds2, y_train_2_trans), getError(train_avg_preds3, y_train_2_trans), getError(train_avg_preds4, y_train_2_trans)))\n",
    "\n",
    "\n",
    "print(\"\\n\\nErrors for Averaged Perceptron:\\nError for 2 rounds: {}\\nError for 3 rounds: {}\\nError for 4 rounds: {}\"\n",
    "      .format(getError(test_avg_preds2, y_test_2_trans), getError(test_avg_preds3, y_test_2_trans), getError(test_avg_preds4, y_test_2_trans)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: {-}\n",
    "    Consider the averaged perceptron classifier w_avg that you built \n",
    "    by running three passes on the data. We will now try to interpret \n",
    "    this classifier.\n",
    "    \n",
    "    Find the three coordinates in w_avg with the highest and lowest \n",
    "    values. What are the words (from pa3dictionary.txt) that \n",
    "    correspond to these coordinates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_largest = list(cls3.w_avg.sort_values(ascending=False).index[:3])\n",
    "train_smallest = list(cls3.w_avg.sort_values().index[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = []\n",
    "with open(\"pa3dictionary.txt\", \"r\") as f:\n",
    "    dictionary = f.read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "a True\n",
      "a True\n",
      "a\n",
      "a True\n",
      "a True\n",
      "b\n",
      "b True\n",
      "b True\n",
      "c\n",
      "c True\n",
      "d\n",
      "d True\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates of largest in w_avg are: [78, 469, 393]\n",
      "Words with highest values are:\n",
      "Highest: he\n",
      "Second Highest: team\n",
      "Third Highest: game\n",
      "\n"
     ]
    }
   ],
   "source": [
    "string = \"Words with highest values are:\\n\"\n",
    "adjectives = [\"Highest\", \"Second Highest\", \"Third Highest\"]\n",
    "i = 0\n",
    "for index in train_largest:\n",
    "    string += adjectives[i] + \": \" + dictionary[index] + \"\\n\"\n",
    "    i+=1\n",
    "    \n",
    "print(\"Coordinates of largest in w_avg are: {}\".format(train_largest))\n",
    "print(string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates of smallest in w_avg are: [438, 466, 203]\n",
      "Words with smallest values are:\n",
      "Least: file\n",
      "Second least: program\n",
      "Third least: line\n",
      "\n"
     ]
    }
   ],
   "source": [
    "string = \"Words with smallest values are:\\n\"\n",
    "adjectives = [\"Least\", \"Second least\", \"Third least\"]\n",
    "i = 0\n",
    "for index in train_smallest:\n",
    "    string += adjectives[i] + \": \" + dictionary[index] + \"\\n\"\n",
    "    i+=1\n",
    "\n",
    "print(\"Coordinates of smallest in w_avg are: {}\".format(train_smallest))\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3:  {-}\n",
    "    Given a test example x, the one-vs-all classifier predicts as follows. If Ci(x) = i for exactly one i = 1,...,6, \n",
    "    then predict label i. If Ci(x) = i for more than one i in 1,...,6, or if Ci(x) = i for no i, then report Don’t \n",
    "    Know\n",
    "    \n",
    "### I have used the value 7 to report don't know, which is also row 6 in the confusion matrix {-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.loadtxt(\"pa3train.txt\")\n",
    "test = np.loadtxt(\"pa3test.txt\")\n",
    "\n",
    "\n",
    "\n",
    "# training data \n",
    "df_train = pd.DataFrame(train)\n",
    "df_test = pd.DataFrame(test)\n",
    "\n",
    "X_train = df_train.drop([819], axis=1)\n",
    "y_train = df_train[819]\n",
    "\n",
    "# create array of labels where each array is 1 for i and -1 for not i where i is 1, 2, 3, ..., 6\n",
    "y_trains_1_to_6 = []\n",
    "for i in range(6):\n",
    "    matchNum = i + 1\n",
    "    y_trains_1_to_6.append(y_train.apply(lambda x: 1 if x == matchNum else -1))\n",
    "    \n",
    "\n",
    "X_test = df_test.drop([819], axis=1)\n",
    "y_test = df_test[819]\n",
    "y_test_1_to_6 = []\n",
    "for i in range(6):\n",
    "    matchNum = i + 1\n",
    "    y_test_1_to_6.append(y_test.apply(lambda x: 1 if x == matchNum else -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 6)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_trains_1_to_6), len(y_test_1_to_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = []\n",
    "for i in range(len(y_trains_1_to_6)):\n",
    "    cls = Perceptron(1)\n",
    "    cls.fit(X_train, y_trains_1_to_6[i])\n",
    "    classifiers.append(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\n",
    "    0: 1, \n",
    "    1: 2, \n",
    "    2: 3, \n",
    "    3: 4, \n",
    "    4: 5, \n",
    "    5: 6\n",
    "}\n",
    "\n",
    "# get predictions on test data and build confusion matrix\n",
    "preds = []\n",
    "for i in range(len(X_test)):\n",
    "    x = X_test.iloc[i]\n",
    "    arr = []\n",
    "    the_class_value = None\n",
    "    for j in range(len(classifiers)):\n",
    "        classifier_w = classifiers[j].w\n",
    "        sign = 0 if classifier_w.dot(x) <= 0 else 1\n",
    "        if sign == 1:\n",
    "            the_class_value = classes[j]\n",
    "        arr.append(sign)\n",
    "    arr = pd.Series(arr)\n",
    "    \n",
    "    if arr.sum() == 1: \n",
    "        preds.append(the_class_value)\n",
    "    elif arr.sum() > 1:\n",
    "        preds.append(7)\n",
    "    elif arr.sum() <= 0:\n",
    "        preds.append(7)\n",
    "    \n",
    "preds = pd.Series(preds)\n",
    "confusion = pd.DataFrame(0, columns=np.arange(6), index=np.arange(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for purpose of matrix i = 1, 2, 3, 4, 5, 6 will map to i = 0,1,2,3,4,5\n",
    "for i in range(len(preds)):\n",
    "    confusion.iloc[preds[i] - 1][y_test[i] - 1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide each j the number of j test examples\n",
    "for i in range(6):\n",
    "    confusion[i] = confusion[i] / y_test.value_counts()[i+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is confusion matrix on test data using the multiclass classifier built on training data {-}\n",
    "#### The index 0, 1, 2, 3, 4, 5 map to i = 1, 2, 3, 4, 5, 6, respectively. There is a 7th row, which is the 'I don't know' row {-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.718919</td>\n",
       "      <td>0.005208</td>\n",
       "      <td>0.034286</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.010811</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.034286</td>\n",
       "      <td>0.027174</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>0.018519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.371429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.016216</td>\n",
       "      <td>0.005208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.690217</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.016216</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.074286</td>\n",
       "      <td>0.005435</td>\n",
       "      <td>0.801282</td>\n",
       "      <td>0.120370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.005405</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.034286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070513</td>\n",
       "      <td>0.490741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.232432</td>\n",
       "      <td>0.276042</td>\n",
       "      <td>0.451429</td>\n",
       "      <td>0.255435</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.342593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5\n",
       "0  0.718919  0.005208  0.034286  0.021739  0.000000  0.000000\n",
       "1  0.010811  0.656250  0.034286  0.027174  0.012821  0.018519\n",
       "2  0.000000  0.015625  0.371429  0.000000  0.000000  0.027778\n",
       "3  0.016216  0.005208  0.000000  0.690217  0.000000  0.000000\n",
       "4  0.016216  0.031250  0.074286  0.005435  0.801282  0.120370\n",
       "5  0.005405  0.010417  0.034286  0.000000  0.070513  0.490741\n",
       "6  0.232432  0.276042  0.451429  0.255435  0.115385  0.342593"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question and Answers to Q3 {-}\n",
    "#### Looking at the confusion matrix, what are the i and j in the following statements? {-}\n",
    "  ##### (a) The perceptron classifier has the highest accuracy for examples that belong to class i. {-}\n",
    "        Answer: Classifier i = 5 (index 4) has the highest accuracy @ .801282 \n",
    "  ##### (b) The perceptron classifier has the least accuracy for examples that belong to class i. {-}\n",
    "        Answer: Classifier i = 3 (index 2) has the least accuracy @ .371429\n",
    "  ##### (c) The perceptron classifier most often mistakenly classifies an example in class j as belonging to class i, for i, j ∈ {1, 2, 3, 4, 5, 6} (i.e., excluding Don’t Know). {-}\n",
    "        Answer: Classiefier i = 5 (index 4) most often mistakenly classifies as 6 (column 5) @ .120370"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
